<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ReGuide</title>

  <!-- Meta tags for social sharing (optional) -->
  <meta name="description" content="DESCRIPTION META TAG" />
  <meta property="og:title" content="ReGuide | ICLR 2025" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:title" content="ReGuide | ICLR 2025" />
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG" />
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />

  <!-- Favicon and Fonts -->
  <link rel="icon" type="image/svg+xml" href="static/images/icon.svg" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.css" />

  <!-- Style -->
   <style>
    .swiper-slide {
      display: flex;
      justify-content: center;
      align-items: center;
      height: auto;
    }

    .swiper-slide img {
      max-width: 100%;
      height: auto;
      display: block;
    }

    .swiper {
      width: 100%;
      height: auto;
    }
  </style>
  
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation</h1>
            <h2 class="title is-3">ICLR 2025</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jihyo-kim-0b5289194/">Jihyo Kim</a><sup>*</sup>,
                <a href="https://www.linkedin.com/in/seulbi-lee-924004342/">Seulbi Lee</a><sup>*</sup>,
                <a href="https://www.linkedin.com/in/sangheum-hwang-99a957a3/">Sangheum Hwang</a>
              </span>
              <p style="font-size: 13px;">(<sup>*</sup> Equal contribution)</p>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Seoul National University of Science and Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=R4h5PXzUuU" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/daintlab/ReGuide" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Github</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/daintlab/reguide" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/huggingface.png" alt="Hugging Face Logo" style="height: 1em; vertical-align: middle;">
                    </span>
                    <span>Huggingface</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section abstract">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p><strong>ReGuide</strong> is a self-guided prompting approach designed to improve 
              <strong>out-of-distribution detection (OoDD) in Large Vision-Language Models (LVLMs)</strong>.
            </p>
            <h3>Motivation</h3>
            <ul>
              <li><strong>Foundation models</strong> (e.g., GPT-4o) are widely used due to their strong generalization abilities.</li>
              <li>However, their <strong>trustworthiness</strong>, particularly in <strong>OoDD scenarios</strong>, is still underexplored.</li>
              <li>This raises concerns about their <strong>safe and reliable deployment</strong> in real-world applications.</li>
            </ul>
          
            <h3>What We Do</h3>
            <ul>
              <li>Evaluate the <strong>OoDD capabilities</strong> of both proprietary and open-source LVLMs.</li>
              <li>Analyze how these models express <strong>confidence</strong> through their <strong>generated natural language responses</strong>.</li>
              <li>Propose <strong>ReGuide</strong>, which:
                <ul>
                  <li>Generates <strong>image-adaptive concept suggestions</strong></li>
                  <li>Improves <strong>confidence estimation</strong> and <strong>robustness</strong> in OoDD settings</li>
                </ul>
              </li>
            </ul>
          
            <h3>Results</h3>
            <ul>
              <li><strong>Improves image classification performance</strong></li>
              <li><strong>Enhances OoDD detection accuracy</strong></li>
              <li>Works as a simple yet effective enhancement for current LVLMs</li>
            </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered">
        <div class="column has-text-centered">
          <figure class="image">
            <img src="static/images/fig1-radar-all.png" alt="Radar All" style="width: 80%; height: auto;">
            <figcaption>
              <p><small>(a) Comparison of LVLMs </small></p>
            </figcaption>
          </figure>
        </div>
        <div class="column has-text-centered">
          <figure class="image">
            <img src="static/images/fig1-radar-reguide.png" alt="Radar ReGuide" style="width: 90%; height: auto;">
            <figcaption>
              <p><small>(b) Effectiveness of ReGuide </small></p>
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">OoDD in LVLMs</h2>
      <!-- framework -->
      <div style="width:70%; margin: 0 auto 1rem auto; text-align: center;">
        <figure style="display: inline-block;">
          <img src="static/images/oodd-in-lvlm.png" alt="OoDD in LVLMs" style="max-width: 100%; height: auto;">
        </figure>
          <p>
            To extend conventional OoD detection (OoDD) to Large Vision-Language Models (LVLMs), this study adapts the zero-shot CLIP-based framework by:
          </p>
            <ol>
              1. Using a fixed set of in-distribution (ID) class names (<code>ùí¥</code>) and a special rejection class ("none of these classes").
              2. Extracting confidence scores from natural language outputs of LVLMs.
              3. Defining the OoD score as the maximum confidence among ID classes applying softmax function for normalization.
            </ol>
          <p>
            The prompt design used in LVLM-based OoDD evaluation consists of four main components:
              <ol>
                <li>
                  <strong>Task Description:</strong>
                  Clearly states the goal, such as ‚ÄúClassify the given image into one of the classes in <code>ùí¥</code>.‚Äù
                </li>
                <li>
                  <strong>Rejection Class:</strong>
                  Adds a special label <code>"none of these classes"</code> to allow the model to reject OoD inputs explicitly.
                </li>
                <li>
                  <strong>Guidelines Addressing Failure Cases:</strong>
                  Ensures robust predictions by:
                  <ul>
                    <li>Avoiding uniform or 0.0 confidence across all classes</li>
                    <li>Requiring a valid score for each class in <code>ùí¥</code></li>
                    <li>Ensuring clear, single prediction outputs</li>
                  </ul>
                </li>
                <li>
                  <strong>Response Examples:</strong>
                  Uses in-context learning (ICL) to present examples of valid responses. This helps LVLMs refer to the correct format and behavior for both ID and OoD inputs.
                </li>
              </ol>
        </p>
      </div>
      <div>
        <p> </p>
      </div>
      <!-- result -->
      <div style="width:100%; margin: 0 auto 1rem auto; text-align: center;">
        <br>
        <figure style="display: inline-block;">
          <img src="static/images/result-oodd-in-lvlms.png" alt="OoDD in LVLMs" style="max-width: 100%; height: auto;">
        </figure>
      </div>
        
  <h3>Key Results and Observations</h3>
  <ul>
    <li>Proprietary LVLMs generally outperform open-source and single-modal models in both ID and OoD tasks.</li>
    <li>LVLMs perform better on <strong>far-OoD</strong> than <strong>near-OoD</strong>.</li>
    <li>InternVL2-76B achieves the best balance across ID accuracy, OoD performance, and valid response rate.</li>
    <li>OpenCLIP struggles with valid outputs but aligns with prior leaderboard rankings.</li>
    <li>Some open-source models show 100% FPR due to biased confidence distribution, e.g., GLM-4v performs best in OoD but poorly in ID classification.</li>
    <li>Prompt quality and rejection class play a crucial role in guiding proper LVLM behavior.</li>
  </ul>
      </div>
    </div>
  </section>
  
  <section class="section result">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Further Analysis</h2> 
      <div class="swiper mySwiper">
        <div class="swiper-wrapper">
          <div class="swiper-slide">
            <img src="static/images/result-further.png" alt="Run 1">
<!--             <br>  -->
          </div>
          <div class="swiper-slide">
            <img src="static/images/result-reasoning.png" alt="Run 2">
          </div>
          <div class="swiper-slide">
            <img src="static/images/result-confidence-id.png" style="max-width:50%", alt="Run 3">
          </div>
        </div>
  
        <!-- Navigation arrows -->
        <div class="swiper-button-next"></div>
        <div class="swiper-button-prev"></div>
  
        <!-- Pagination dots (optional) -->
        <div class="swiper-pagination"></div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Reflexive Guidance</h2>
      <div class="has-text-centered" style="margin-bottom: 1rem; text-align: center;">
        <figure class="image is-inline-block">
          <img src="static/images/reguide.png" alt="ReGuide Diagram">
        </figure>
      </div>
      <div>
        <p> Reflexive Guidance (ReGuide) is a simple and model-agnostic two-stage prompting strategy to enhance the OoD detectability of LVLMs. We leverage the LVLM itself to obtain guidance for OoDD from its powerful zero-shot visual recognition capabilities.</p>
        <br>
        <p> <b>Stage 1</b> The LVLM is asked to suggest <i>2N</i> class names derived from the given image. The first <i>N</i> classes are visually similar to the image conceptually corresponding to the near-OoD. The second <i>N</i> classes are visually dissimilar or belong to different domains conceptually corresponding to far-OoD. For OoD images, suggested classes often align closely with the ground-truth label. </p>
        <br>
        <p> <b>Stage 2</b> The suggested classes are employed as auxiliary OoD classes. OoD inputs may receive higher confidence score for suggested than for ID classes. The rejection class is retained as a fallback.
      </div>
      <!-- result -->
      <div style="width:100%; margin: 0 auto 1rem auto; text-align: center;">
        <br>
        <figure style="display: inline-block;">
          <img src="static/images/result-reguide.png" alt="OoDD in LVLMs" style="max-width: 100%; height: auto;">
        </figure>
      </div>
      <div>
        <p> ID classification accuracy improves even though the suggested classes includes class names that are visually similar to ID (i.e., near-OoD). InternVL2-26B shows improvements in both FPR@95%TPR and AUROC, but larger models like InternVL2-76B and GPT-4o exhibit improved AUROC but degraded FPR@95%TPR. This is due to labeling issues on a small subset of ID. </p>
      </div>
    </div>
  </section>

  
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">BibTeX</h2>
      <div style="background: #f5f5f5; padding: 1em; border-radius: 6px; font-family: monospace;">
        @inproceedings{<br>
        &nbsp;&nbsp;kim2025reflexive,<br>
        &nbsp;&nbsp;title={Reflexive Guidance: Improving Oo{DD} in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation},<br>
        &nbsp;&nbsp;author={Jihyo Kim and Seulbi Lee and Sangheum Hwang},<br>
        &nbsp;&nbsp;booktitle={The Thirteenth International Conference on Learning Representations},<br>
        &nbsp;&nbsp;year={2025},<br>
        &nbsp;&nbsp;url={https://openreview.net/forum?id=R4h5PXzUuU}<br>
        }
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the design of this website, we just ask that you link back to this page in the footer.
              <br />
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>
              .
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.js"></script>
  <script>
  const swiper = new Swiper('.mySwiper', {
    loop: true,
    navigation: {
      nextEl: '.swiper-button-next',
      prevEl: '.swiper-button-prev',
    },
    pagination: {
      el: '.swiper-pagination',
      clickable: true,
    },
  });
  </script>

  <script>
  const swiper = new Swiper('.mySwiper', {
    loop: true,
    autoHeight: true, // ‚úÖ Add this line
    navigation: {
      nextEl: '.swiper-button-next',
      prevEl: '.swiper-button-prev',
    },
    pagination: {
      el: '.swiper-pagination',
      clickable: true,
    }
  });
  </script>
  
</body>
</html>
