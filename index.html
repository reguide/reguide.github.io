<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ReGuide</title>

  <!-- Meta tags for social sharing (optional) -->
  <meta name="description" content="DESCRIPTION META TAG" />
  <meta property="og:title" content="ReGuide | ICLR 2025" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:title" content="ReGuide | ICLR 2025" />
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG" />
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />

  <!-- Favicon and Fonts -->
  <link rel="icon" type="image/svg+xml" href="static/images/icon.svg" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <!-- Stylesheets -->
  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation</h1>
            <h2 class="title is-3">ICLR 2025</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jihyo-kim-0b5289194/">Jihyo Kim</a><sup>*</sup>,
                <a href="https://www.linkedin.com/in/seulbi-lee-924004342/">Seulbi Lee</a><sup>*</sup>,
                <a href="https://www.linkedin.com/in/sangheum-hwang-99a957a3/">Sangheum Hwang</a>
              </span>
              <p style="font-size: 13px;">(<sup>*</sup> Equal contribution)</p>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Seoul National University of Science and Technology</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=R4h5PXzUuU" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/daintlab/ReGuide" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Github</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/daintlab/reguide" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/huggingface.png" alt="Hugging Face Logo" style="height: 1em; vertical-align: middle;">
                    </span>
                    <span>Huggingface</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section abstract">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              With the recent emergence of foundation models trained on internet-scale data and demonstrating remarkable generalization capabilities, such foundation models have become more widely adopted, leading to an expanding range of application domains. Despite this rapid proliferation, the trustworthiness of foundation models remains underexplored. Specifically, the out-of-distribution detection (OoDD) capabilities of large vision-language models (LVLMs), such as GPT-4o, which are trained on massive multi-modal data, have not been sufficiently addressed. The disparity between their demonstrated potential and practical reliability raises concerns regarding the safe and trustworthy deployment of foundation models. To address this gap, we evaluate and analyze the OoDD capabilities of various proprietary and open-source LVLMs. Our investigation contributes to a better understanding of how these foundation models represent confidence scores through their generated natural language responses. Furthermore, we propose a self-guided prompting approach, termed Reflexive Guidance (ReGuide), aimed at enhancing the OoDD capability of LVLMs by leveraging self-generated image-adaptive concept suggestions. Experimental results demonstrate that our ReGuide enhances the performance of current LVLMs in both image classification and OoDD tasks. The lists of sampled images, along with the prompts and responses for each sample are available at <a href="https://github.com/daintlab/ReGuide" target="_blank">https://github.com/daintlab/ReGuide</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered">
        <div class="column has-text-centered">
          <figure class="image">
            <img src="static/images/fig1-radar-all.png" alt="Radar All" style="width: 80%; height: auto;">
            <figcaption>
              <p><small>(a) Comparison of LVLMs </small></p>
            </figcaption>
          </figure>
        </div>
        <div class="column has-text-centered">
          <figure class="image">
            <img src="static/images/fig1-radar-reguide.png" alt="Radar ReGuide" style="width: 90%; height: auto;">
            <figcaption>
              <p><small>(b) Effectiveness of ReGuide </small></p>
            </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">OoDD in LVLMs</h2>
      <div style="width:70%; margin: 0 auto 1rem auto; text-align: center;">
        <figure style="display: inline-block;">
          <img src="static/images/oodd-in-lvlm.png" alt="OoDD in LVLMs" style="max-width: 100%; height: auto;">
        </figure>
      </div>
      <div>
        <p> Given the vast amount and broad domain coverage of data used to train LVLMs, the conventional definition faces challenges in its direct application to LVLMs. To address this, we extend the zero-shot OoDD framework of CLIP to generative LVLMs: the scenario where an in-distribution (ID) class words set does not contain the ground-truth label of an input image. We use the maximum confidence score among ID classes as OoD score.</p>
      </div>
    </div>
  </section>

  <section class="section result">
    <div class="container">
      <div class="carousel carousel-animated carousel-animate-slide" data-autoplay="true">
        <div class="carousel-container">
          <div class="carousel-item is-active">
            <img src="static/images/oodd-in-lvlm.png" alt="Run 1">
          </div>
          <div class="carousel-item">
            <img src="static/images/reguide.png" alt="Run 2">
          </div>
          <div class="carousel-item">
            <img src="static/images/fig1-radar-all.png" alt="Run 3">
          </div>
        </div>
        <div class="carousel-navigation is-overlay">
          <div class="carousel-nav-left">
            <i class="fa fa-chevron-left" aria-hidden="true"></i>
          </div>
          <div class="carousel-nav-right">
            <i class="fa fa-chevron-right" aria-hidden="true"></i>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">ReGuide</h2>
      <div class="has-text-centered" style="margin-bottom: 1rem; text-align: center;">
        <figure class="image is-inline-block">
          <img src="static/images/reguide.png" alt="ReGuide Diagram">
        </figure>
      </div>
      <div>
        <p> Reflexive Guidance (ReGuide) is a simple and model-agnostic two-stage prompting strategy to enhance the OoD detectability of LVLMs. We leverage the LVLM itself to obtain guidance for OoDD from its powerful zero-shot visual recognition capabilities.</p>
        <br>
        <p> <b>Stage 1</b> The LVLM is asked to suggest <i>2N</i> class names derived from the given image. The first <i>N</i> classes are visually similar to the image conceptually corresponding to the near-OoD. The second <i>N</i> classes are visually dissimilar or belong to different domains conceptually corresponding to far-OoD. For OoD images, suggested classes often align closely with the ground-truth label. </p>
        <br>
        <p> <b>Stage 2</b> The suggested classes are employed as auxiliary OoD classes. OoD inputs may receive higher confidence score for suggested than for ID classes. The rejection class is retained as a fallback.
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">BibTeX</h2>
      <div style="background: #f5f5f5; padding: 1em; border-radius: 6px; font-family: monospace;">
        @inproceedings{<br>
        &nbsp;&nbsp;kim2025reflexive,<br>
        &nbsp;&nbsp;title={Reflexive Guidance: Improving Oo{DD} in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation},<br>
        &nbsp;&nbsp;author={Jihyo Kim and Seulbi Lee and Sangheum Hwang},<br>
        &nbsp;&nbsp;booktitle={The Thirteenth International Conference on Learning Representations},<br>
        &nbsp;&nbsp;year={2025},<br>
        &nbsp;&nbsp;url={https://openreview.net/forum?id=R4h5PXzUuU}<br>
        }
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
                Academic Project Page Template
              </a>
              which was adopted from the
              <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
              project page. You are free to borrow the design of this website, we just ask that you link back to this page in the footer.
              <br />
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                Creative Commons Attribution-ShareAlike 4.0 International License
              </a>
              .
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/js/bulma-carousel.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      bulmaCarousel.attach('.carousel', {
        slidesToScroll: 1,
        slidesToShow: 1,
        loop: true
      });
    });
  </script>
</body>
</html>
